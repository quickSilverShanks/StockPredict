{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "import string\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>Change</th>\n",
       "      <th>Change_percent</th>\n",
       "      <th>Mrk_Cap(Rs Cr)</th>\n",
       "      <th>Scrape_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adani Ports</td>\n",
       "      <td>Transport Infrastructure</td>\n",
       "      <td>361.55</td>\n",
       "      <td>7.90</td>\n",
       "      <td>2.23</td>\n",
       "      <td>73,457.98</td>\n",
       "      <td>2020-11-04 15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Paints</td>\n",
       "      <td>2,169.95</td>\n",
       "      <td>15.30</td>\n",
       "      <td>0.71</td>\n",
       "      <td>208,141.12</td>\n",
       "      <td>2020-11-04 15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Axis Bank</td>\n",
       "      <td>Bank - Private</td>\n",
       "      <td>520.05</td>\n",
       "      <td>-14.10</td>\n",
       "      <td>-2.64</td>\n",
       "      <td>159,145.18</td>\n",
       "      <td>2020-11-04 15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bajaj Auto</td>\n",
       "      <td>Automobile - 2 &amp; 3 Wheelers</td>\n",
       "      <td>2,930.00</td>\n",
       "      <td>15.15</td>\n",
       "      <td>0.52</td>\n",
       "      <td>84,784.54</td>\n",
       "      <td>2020-11-04 15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bajaj Finance</td>\n",
       "      <td>Finance - NBFC</td>\n",
       "      <td>3,561.00</td>\n",
       "      <td>70.20</td>\n",
       "      <td>2.01</td>\n",
       "      <td>214,581.35</td>\n",
       "      <td>2020-11-04 15:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Company_Name                     Industry Last_Price  Change  \\\n",
       "0    Adani Ports     Transport Infrastructure     361.55    7.90   \n",
       "1   Asian Paints                       Paints   2,169.95   15.30   \n",
       "2      Axis Bank               Bank - Private     520.05  -14.10   \n",
       "3     Bajaj Auto  Automobile - 2 & 3 Wheelers   2,930.00   15.15   \n",
       "4  Bajaj Finance               Finance - NBFC   3,561.00   70.20   \n",
       "\n",
       "  Change_percent Mrk_Cap(Rs Cr)         Scrape_date  \n",
       "0           2.23      73,457.98 2020-11-04 15:30:00  \n",
       "1           0.71     208,141.12 2020-11-04 15:30:00  \n",
       "2          -2.64     159,145.18 2020-11-04 15:30:00  \n",
       "3           0.52      84,784.54 2020-11-04 15:30:00  \n",
       "4           2.01     214,581.35 2020-11-04 15:30:00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the latest Nifty50 Index Composition with previous day details, such as, company names, prices, change etc.\n",
    "\n",
    "getnifty50 = \"https://www.moneycontrol.com/stocks/marketstats/indexcomp.php?optex=NSE&opttopic=indexcomp&index=9\"\n",
    "soup = BeautifulSoup(get(getnifty50).text, 'lxml')\n",
    "composition_n50 = soup.select('table.tbldata14.bdrtpg')[0]\n",
    "\n",
    "Scrape_date = parser.parse(soup.select('div.FR.b_15.PT5')[0].text)\n",
    "Company_Name = [script.text.strip() for script in composition_n50.select(\"a.bl_12\")[2::2]]\n",
    "Industry = [script.text.strip() for script in composition_n50.select(\"a.bl_12\")[3::2]]\n",
    "# urlsplit = [script.get('href').split('/')[-1] for script in composition_n50.select(\"a.bl_12\")[2::2]]\n",
    "\n",
    "Last_Price = [script.text.strip() for script in composition_n50.select('td.brdrgtgry')[2::6]]\n",
    "Change = [script.text.strip() for script in composition_n50.select('td.brdrgtgry')[3::6]]\n",
    "Change_percent = [script.text.strip() for script in composition_n50.select('td.brdrgtgry')[4::6]]\n",
    "Mrk_Cap = [script.text.strip() for script in composition_n50.select('td.brdrgtgry')[5::6]]\n",
    "\n",
    "\n",
    "nifty50_latest = pd.DataFrame({\n",
    "    'Company_Name' : Company_Name,\n",
    "    'Industry' : Industry,\n",
    "#     'urlsplit' : urlsplit,\n",
    "    'Last_Price' : Last_Price,\n",
    "    'Change' : Change,\n",
    "    'Change_percent' : Change_percent,\n",
    "    'Mrk_Cap(Rs Cr)' : Mrk_Cap\n",
    "})\n",
    "\n",
    "nifty50_latest['Scrape_date'] = Scrape_date\n",
    "print(nifty50_latest.shape)\n",
    "nifty50_latest.to_csv(\"nifty50_latest.csv\")\n",
    "nifty50_latest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr.No.</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Weightage</th>\n",
       "      <th>thehindu_searchstring</th>\n",
       "      <th>mcontrol_substring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reliance Industries Ltd.</td>\n",
       "      <td>Petroleum Products</td>\n",
       "      <td>14.93%</td>\n",
       "      <td>reliance%20petroleum</td>\n",
       "      <td>RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HDFC Bank Ltd.</td>\n",
       "      <td>Banks</td>\n",
       "      <td>9.69%</td>\n",
       "      <td>hdfc%20bank</td>\n",
       "      <td>HDF01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Infosys Limited</td>\n",
       "      <td>Software</td>\n",
       "      <td>7.63%</td>\n",
       "      <td>infosys</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Housing Development Fin. Corp. Ltd.</td>\n",
       "      <td>Finance</td>\n",
       "      <td>6.44%</td>\n",
       "      <td>hdfc</td>\n",
       "      <td>HDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tata Consultancy Services Ltd.</td>\n",
       "      <td>Software</td>\n",
       "      <td>5.41%</td>\n",
       "      <td>tcs</td>\n",
       "      <td>TCS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr.No.                         Company Name              Sector Weightage  \\\n",
       "0       1             Reliance Industries Ltd.  Petroleum Products    14.93%   \n",
       "1       2                       HDFC Bank Ltd.               Banks     9.69%   \n",
       "2       3                      Infosys Limited            Software     7.63%   \n",
       "3       4  Housing Development Fin. Corp. Ltd.             Finance     6.44%   \n",
       "4       5       Tata Consultancy Services Ltd.            Software     5.41%   \n",
       "\n",
       "  thehindu_searchstring mcontrol_substring  \n",
       "0  reliance%20petroleum                 RI  \n",
       "1           hdfc%20bank              HDF01  \n",
       "2               infosys                 IT  \n",
       "3                  hdfc                HDF  \n",
       "4                   tcs                TCS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup Table with Nifty50 stocks and MoneyControl url sub-strings\n",
    "nifty50_lookuptable = pd.read_csv(\"nifty50_lookuptable.csv\")\n",
    "Substring = [i for i in nifty50_lookuptable['mcontrol_substring']]\n",
    "# cnames = [i for i in nifty50_lookuptable['Company Name']]\n",
    "print(nifty50_lookuptable.shape)\n",
    "nifty50_lookuptable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(yr, urlsplit):\n",
    "    '''\n",
    "    Function to obtain total number of result pages, initialize blank news data and\n",
    "    set urls for moneycontrol news search page for the input year 'yr'.\n",
    "    '''\n",
    "    global ticker, url_all, headlines, dates, news, urls, sources\n",
    "    \n",
    "    urlyr = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\" + urlsplit + \"&durationType=Y&Year={}\"\n",
    "    url = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id={}&scat=&pageno={}&next=0&durationType=Y&Year={}&duration=1&news_type=\"\n",
    "    \n",
    "    soup = BeautifulSoup(get(urlyr.format(yr)).text, 'lxml')\n",
    "    ticker = soup.select('div.FL.gry10')[0].text.split('|')[1].split(':')[1].strip()\n",
    "    result_max = len(soup.select('div.pages.MR10.MT15')[0].select('a')) + 1\n",
    "    \n",
    "    url_all = [url.format(urlsplit, i, yr) for i in range(1, result_max+1)]\n",
    "    headlines, dates, news, urls, sources = [], [], [], [], []\n",
    "    print(\"Total number of result pages for\", ticker, \"in the year\", yr, \":\", result_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnewslinks():\n",
    "    '''\n",
    "    Function to scrape news headlines, urls, publish dates etc.\n",
    "    '''\n",
    "    print(\"[INFO] Extracting Links...\")\n",
    "\n",
    "    for src in tqdm(url_all):\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(get(src).text, 'lxml')\n",
    "\n",
    "            # Extracts the Headlines\n",
    "            try:\n",
    "                headline = [script.text.strip() for script in soup.select('a.g_14bl')]\n",
    "                headlines.extend(headline)\n",
    "            except:\n",
    "                print('Exception in Headline')\n",
    "                headlines.extend(None)\n",
    "\n",
    "            # Extracts the urls\n",
    "            try:\n",
    "                source = [\"https://www.moneycontrol.com\"+script.get('href') for script in soup.select('a.g_14bl')]\n",
    "                urls.extend(source)\n",
    "            except:\n",
    "                print('Exception in url')\n",
    "                urls.extend(None)\n",
    "\n",
    "            # Extracts the published dates\n",
    "            try:\n",
    "                dateline = [str(parser.parse(script.text.split('|')[1].strip())).split()[0] for script in soup.select('p.PT3.a_10dgry')]\n",
    "                dates.extend(dateline)\n",
    "            except:\n",
    "                print('Exception in dateline')\n",
    "                dates.extend(None)\n",
    "\n",
    "            # Extracts the bylines\n",
    "            try:\n",
    "                bylines = [script.select('span.a_2_10bl')[0].text.strip() if len(script.select('span.a_2_10bl'))==1 else None\n",
    "                           for script in soup.select('p.PT3.a_10dgry')]\n",
    "                sources.extend(bylines)\n",
    "            except:\n",
    "                print('Exception in bylines')\n",
    "                sources.extend(None)\n",
    "\n",
    "        except:\n",
    "            print(\"Exception occurred in url : \", src)\n",
    "            break\n",
    "\n",
    "    print(\"[INFO] Links Extracted.\")\n",
    "    print(\"Total No. of Pages to be Scraped = \", len(urls))\n",
    "    print(\"Oldest Available Article: \", min(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticles(thres=20):\n",
    "    '''\n",
    "    Function to scrape news articles. Any paragraph with words less than 'thres' will not be considered.\n",
    "    '''\n",
    "    print(\"[INFO] Extracting Articles...\")\n",
    "\n",
    "    for src in tqdm(urls):\n",
    "        try:\n",
    "            # Parse the url to NewsPage\n",
    "            soup = BeautifulSoup(get(src).text, 'lxml')\n",
    "\n",
    "            # Extracts the news articles\n",
    "            try:\n",
    "                news_article = '.'.join([scrape.text.strip() for scrape in soup.select(\"div.arti-flow\")[0].select(\"p\")\n",
    "                                         if len(scrape.text.split()) >= thres])\n",
    "                news.append(news_article)\n",
    "            except:\n",
    "                news.append(None)\n",
    "\n",
    "        except:\n",
    "            print(\"Exception occurred in url : \", src)\n",
    "            news.append(None)\n",
    "\n",
    "    print(\"[INFO] Articles Extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chkdata():\n",
    "    '''\n",
    "    Function to check for any missing values in the Dataframe and drop it.\n",
    "    '''\n",
    "    global df\n",
    "    df = pd.DataFrame({'Headlines': headlines,\n",
    "                       'Articles': news,\n",
    "                       'Published_Dates': dates,\n",
    "                       'Source_URLs': urls,\n",
    "                       'ByLines' : sources\n",
    "                       })\n",
    "    print(\"Missing Info in Scraped Data :\")\n",
    "    print(df.isna().sum())\n",
    "    df=df.dropna(axis = 0)\n",
    "    print(\"Total Usable Scraped Data : \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefile(tickr,yr):\n",
    "    '''\n",
    "    Function to save the scraped data as pickle file.\n",
    "    '''\n",
    "    # df.to_csv(\"news_mcontrol_\"+ tickr + \"_\" + str(yr) + \".csv\")\n",
    "    df.to_pickle(\"news_mcontrol_\"+ tickr + \"_\" + str(yr) + \".pkl\")\n",
    "    print(\"Data saved for\", tickr, \"for year\",yr, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nifty50 Extraction Search Count : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for RELIANCE in the year 2019 : 7\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:47<00:00,  6.82s/it]\n",
      "  0%|                                                                                          | 0/138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  138\n",
      "Oldest Available Article:  2019-01-06\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 138/138 [04:28<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Articles           138\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (0, 5)\n",
      "Data saved for RELIANCE for year 2019 .\n",
      "Nifty50 Extraction Search Count : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for HDFCBANK in the year 2019 : 5\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.79s/it]\n",
      "  0%|                                                                                           | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  92\n",
      "Oldest Available Article:  2019-01-14\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [02:51<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Articles           92\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (0, 5)\n",
      "Data saved for HDFCBANK for year 2019 .\n",
      "Nifty50 Extraction Search Count : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for INFY in the year 2019 : 7\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:40<00:00,  5.80s/it]\n",
      "  0%|                                                                                          | 0/132 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  132\n",
      "Oldest Available Article:  2019-01-04\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 132/132 [05:27<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Articles           131\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (1, 5)\n",
      "Data saved for INFY for year 2019 .\n",
      "Nifty50 Extraction Search Count : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for HDFC in the year 2019 : 3\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.95s/it]\n",
      "  0%|                                                                                           | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  57\n",
      "Oldest Available Article:  2019-01-14\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 57/57 [02:07<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Articles           57\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (0, 5)\n",
      "Data saved for HDFC for year 2019 .\n",
      "Nifty50 Extraction Search Count : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for TCS in the year 2019 : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:30<00:00,  5.09s/it]\n",
      "  0%|                                                                                          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  110\n",
      "Oldest Available Article:  2019-01-08\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [03:49<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Articles           108\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (2, 5)\n",
      "Data saved for TCS for year 2019 .\n",
      "Nifty50 Extraction Search Count : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ICICIBANK in the year 2019 : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:26<00:00,  4.35s/it]\n",
      "  0%|                                                                                          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  110\n",
      "Oldest Available Article:  2019-01-02\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [03:53<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Articles           110\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (0, 5)\n",
      "Data saved for ICICIBANK for year 2019 .\n",
      "Nifty50 Extraction Search Count : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for KOTAKBANK in the year 2019 : 4\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:24<00:00,  6.16s/it]\n",
      "  0%|                                                                                           | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  62\n",
      "Oldest Available Article:  2019-01-01\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 62/62 [02:33<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Articles           61\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (1, 5)\n",
      "Data saved for KOTAKBANK for year 2019 .\n",
      "Nifty50 Extraction Search Count : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for HINDUNILVR in the year 2019 : 5\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.35s/it]\n",
      "  0%|                                                                                           | 0/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  88\n",
      "Oldest Available Article:  2019-01-10\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [03:20<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bb20c0e8539d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mgetnewslinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mgetarticles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mchkdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0msavefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7fe7dbab8029>\u001b[0m in \u001b[0;36mchkdata\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                        \u001b[1;34m'Published_Dates'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                        \u001b[1;34m'Source_URLs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                        \u001b[1;34m'ByLines'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                        })\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Missing Info in Scraped Data :\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\stockpredict\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\stockpredict\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\stockpredict\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\stockpredict\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# Scraping 2019 news articles for all the companies listed in Nifty50\n",
    "\n",
    "yr = 2019\n",
    "\n",
    "for i, sstring in enumerate(Substring):\n",
    "#     if i>=0 and i<=9:\n",
    "        print(\"Nifty50 Extraction Search Count :\",i+1)\n",
    "        initialize(yr, sstring)\n",
    "        getnewslinks()\n",
    "        getarticles()\n",
    "        chkdata()\n",
    "        savefile(ticker, yr)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pages with blank Articles might have a different structure.\n",
    "# There is a text '\\xa0' in the articles that was '&nbsp;' in the html. This will be replaced by ' ' while preprocessing.\n",
    "# Some pages have no byline. Of these, some have video articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
