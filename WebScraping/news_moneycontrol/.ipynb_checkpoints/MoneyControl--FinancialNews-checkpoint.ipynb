{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "import string\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>urlsplit1</th>\n",
       "      <th>urlsplit2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HDFC Bank</td>\n",
       "      <td>Bank - Private</td>\n",
       "      <td>hdfcbank</td>\n",
       "      <td>HDF01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HDFC</td>\n",
       "      <td>Finance - Housing</td>\n",
       "      <td>hdfc</td>\n",
       "      <td>HDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICICI Bank</td>\n",
       "      <td>Bank - Private</td>\n",
       "      <td>icicibank</td>\n",
       "      <td>ICI02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infosys</td>\n",
       "      <td>IT Services &amp; Consulting</td>\n",
       "      <td>infosys</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kotak Mahindra</td>\n",
       "      <td>Bank - Private</td>\n",
       "      <td>kotakmahindra</td>\n",
       "      <td>KMB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company_Name                  Industry      urlsplit1 urlsplit2\n",
       "0       HDFC Bank            Bank - Private       hdfcbank     HDF01\n",
       "1            HDFC         Finance - Housing           hdfc       HDF\n",
       "2      ICICI Bank            Bank - Private      icicibank     ICI02\n",
       "3         Infosys  IT Services & Consulting        infosys        IT\n",
       "4  Kotak Mahindra            Bank - Private  kotakmahindra       KMB"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the latest Nifty50 company names, prices, change etc.\n",
    "# At the moment, this is only scraping info required for news scraping, i'll scrape prices etc. at a later stage.\n",
    "\n",
    "getnifty50 = \"https://www.moneycontrol.com/stocks/marketstats/indcontrib.php?optex=NSE&opttopic=indcontrib&index=9\"\n",
    "soup = BeautifulSoup(get(getnifty50).text, 'lxml')\n",
    "Company_Name = [script.text.strip() for script in soup.select(\"a.bl_12\")[0::2]]\n",
    "Industry = [script.text.strip() for script in soup.select(\"a.bl_12\")[1::2]]\n",
    "\n",
    "urlsplit1 = [script.get('href').split('/')[-2] for script in soup.select(\"a.bl_12\")[0::2]]\n",
    "urlsplit2 = [script.get('href').split('/')[-1] for script in soup.select(\"a.bl_12\")[0::2]]\n",
    "\n",
    "nifty50_lookuptable = pd.DataFrame({\n",
    "    'Company_Name' : Company_Name,\n",
    "    'Industry' : Industry,\n",
    "    'urlsplit1' : urlsplit1,\n",
    "    'urlsplit2' : urlsplit2\n",
    "})\n",
    "\n",
    "print(nifty50_lookuptable.shape)\n",
    "nifty50_lookuptable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(urlsplit2):\n",
    "    '''\n",
    "    Function to obtain total number of result pages, initialize blank news data and\n",
    "    set urls for moneycontrol news search page for 2020 and 2019.\n",
    "    '''\n",
    "    global ticker, url_all, headlines, dates, news, urls, sources\n",
    "    \n",
    "    url = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id={}&scat=&pageno={}&next=0&durationType=Y&Year={}&duration=1&news_type=\"\n",
    "    url19 = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\" + urlsplit2 + \"&durationType=Y&Year=2019\"\n",
    "    url20 = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\" + urlsplit2 + \"&durationType=Y&Year=2020\"\n",
    "    \n",
    "    soup = BeautifulSoup(get(url19).text, 'lxml')\n",
    "    result_max = [len(soup.select('div.pages.MR10.MT15')[0].select('a')) + 1]\n",
    "    \n",
    "    soup = BeautifulSoup(get(url20).text, 'lxml')\n",
    "    ticker = soup.select('div.FL.gry10')[0].text.split('|')[1].split(':')[1].strip()\n",
    "    result_max += [len(soup.select('div.pages.MR10.MT15')[0].select('a')) + 1]\n",
    "    \n",
    "    url_all = [url.format(ticker, i, 2019) for i in range(1, result_max[0]+1)] + [url.format(ticker, i, 2020) for i in range(1, result_max[0]+1)]\n",
    "    headlines, dates, news, urls, sources = [], [], [], [], []\n",
    "    print(\"Total number of result pages for\", ticker, \":\", result_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet to update the below codes for MoneyControl. Above part of the notebook code is already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnewslinks():\n",
    "    '''\n",
    "    Function to scrape news headlines, urls, publish dates etc.\n",
    "    '''\n",
    "    print(\"[INFO] Extracting Links...\")\n",
    "\n",
    "    for src in tqdm(url_all):\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(get(src).text, 'lxml')\n",
    "\n",
    "            # Extracts the Headlines\n",
    "            try:\n",
    "                headline = [script.text.strip() for script in soup.select('a.story-card75x1-text')]\n",
    "                headlines.extend(headline)\n",
    "            except:\n",
    "                headlines.extend(None)\n",
    "\n",
    "            # Extracts the urls\n",
    "            try:\n",
    "                source = [script.get('href') for script in soup.select('a.story-card75x1-text')]\n",
    "                urls.extend(source)\n",
    "            except:\n",
    "                urls.extend(None)\n",
    "\n",
    "            # Extracts the sections(markets, industry, business etc.)\n",
    "            try:\n",
    "                section = [script.text.strip() for script in soup.select('a.section-name')]\n",
    "                sections.extend(section)\n",
    "            except:\n",
    "                sections.extend(None)\n",
    "\n",
    "            # Extracts the published dates\n",
    "            try:\n",
    "                dateline = [str(parser.parse(script.text)).split()[0] for script in soup.select('span.dateline')]\n",
    "                dates.extend(dateline)\n",
    "            except:\n",
    "                dates.extend(None)\n",
    "\n",
    "            # Extracts the bylines\n",
    "    #         try:\n",
    "    #             bylines = [script.text.strip() for script in soup.select('a.story-card-33-author-name')]\n",
    "    #             authors.extend(bylines)\n",
    "    #             assert len(bylines)==12\n",
    "    #         except:\n",
    "    #             authors.extend(None)\n",
    "\n",
    "        except:\n",
    "            print(\"Exception occurred in url : \", src)\n",
    "            break\n",
    "\n",
    "    print(\"[INFO] Links Extracted.\")\n",
    "    print(\"Total No. of Pages to be Scraped = \", len(urls))\n",
    "    print(\"Oldest Available Article: \", min(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticles():\n",
    "    '''\n",
    "    Function to scrape news articles and bylines.\n",
    "    '''\n",
    "    print(\"[INFO] Extracting Articles...\")\n",
    "\n",
    "    for src in tqdm(urls):\n",
    "        try:\n",
    "            # Parse the url to NewsPage\n",
    "            soup = BeautifulSoup(get(src).text, 'lxml')\n",
    "\n",
    "            # Extracts the news articles\n",
    "            try:\n",
    "                news_article = soup.find(id='content-body-14269002-' + re.findall(r\"\\d+\",src.split('/')[-1])[0]).text.strip()\n",
    "                news.append(news_article)\n",
    "            except:\n",
    "                news.append(None)\n",
    "\n",
    "            # Extracts the bylines\n",
    "            try:\n",
    "                bylines = [script.text.strip() for script in soup.select('a.auth-nm')]\n",
    "                authors.extend([' | '.join(bylines)])\n",
    "            except:\n",
    "                authors.extend(None)\n",
    "\n",
    "        except:\n",
    "            print(\"Exception occurred in url : \", src)\n",
    "            news.append(None)\n",
    "\n",
    "    print(\"[INFO] Articles Extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chkdata():\n",
    "    '''\n",
    "    Function to check for any missing values in the Dataframe and drop it.\n",
    "    '''\n",
    "    global df\n",
    "    df = pd.DataFrame({'Headlines': headlines,\n",
    "                       'Sections' : sections,\n",
    "                       'Articles': news,\n",
    "                       'Published_Dates': dates,\n",
    "                       'Source_URLs': urls,\n",
    "                       'ByLines' : authors\n",
    "                       })\n",
    "    print(\"Missing Info in Scraped Data :\")\n",
    "    print(df.isna().sum())\n",
    "    df=df.dropna(axis = 0)\n",
    "    print(\"Total Usable Scraped Data : \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefile(SearchString):\n",
    "    '''\n",
    "    Function to save the scraped data as pickle file.\n",
    "    '''\n",
    "    sname = '_'.join(SearchString.split('%20'))\n",
    "    # df.to_csv(\"news_thehindu_\"+ sname + \".csv\")\n",
    "    df.to_pickle(\"news_thehindu_\"+ sname + \".pkl\")\n",
    "    print(\"Data saved for\", SearchString, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nifty50 Extraction Search Count : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for reliance%20petroleum : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.56s/it]\n",
      "  2%|█▎                                                                                 | 1/63 [00:00<00:12,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  63\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           52\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (11, 6)\n",
      "Data saved for reliance%20petroleum .\n",
      "Nifty50 Extraction Search Count : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hdfc%20bank : 49\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49/49 [01:13<00:00,  1.50s/it]\n",
      "  0%|                                                                                          | 0/586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  586\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 586/586 [07:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           137\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (449, 6)\n",
      "Data saved for hdfc%20bank .\n",
      "Nifty50 Extraction Search Count : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for infosys : 32\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:47<00:00,  1.47s/it]\n",
      "  0%|▏                                                                                 | 1/383 [00:00<01:15,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  383\n",
      "Oldest Available Article:  2019-10-21\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 383/383 [02:47<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           83\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (300, 6)\n",
      "Data saved for infosys .\n",
      "Nifty50 Extraction Search Count : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hdfc : 61\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 61/61 [01:29<00:00,  1.46s/it]\n",
      "  0%|                                                                                  | 1/725 [00:00<02:24,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  725\n",
      "Oldest Available Article:  2019-10-21\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 725/725 [03:59<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           137\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (588, 6)\n",
      "Data saved for hdfc .\n",
      "Nifty50 Extraction Search Count : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for tcs : 28\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:39<00:00,  1.39s/it]\n",
      "  0%|                                                                                          | 0/330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  330\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 330/330 [02:19<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           72\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (258, 6)\n",
      "Data saved for tcs .\n",
      "Nifty50 Extraction Search Count : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ICICI%20BANK : 37\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:53<00:00,  1.44s/it]\n",
      "  0%|                                                                                          | 0/443 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  443\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 443/443 [03:28<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           117\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (326, 6)\n",
      "Data saved for ICICI%20BANK .\n",
      "Nifty50 Extraction Search Count : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for kotak%20bank : 28\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:40<00:00,  1.44s/it]\n",
      "  0%|                                                                                          | 0/326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  326\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 326/326 [02:05<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           91\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (235, 6)\n",
      "Data saved for kotak%20bank .\n",
      "Nifty50 Extraction Search Count : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hul : 17\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:23<00:00,  1.41s/it]\n",
      "  0%|▍                                                                                 | 1/200 [00:00<00:39,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  200\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:05<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           49\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (151, 6)\n",
      "Data saved for hul .\n",
      "Nifty50 Extraction Search Count : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for itc : 20\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:29<00:00,  1.48s/it]\n",
      "  0%|                                                                                          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  239\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 239/239 [01:18<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           55\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (184, 6)\n",
      "Data saved for itc .\n",
      "Nifty50 Extraction Search Count : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for larsen%20toubro : 3\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.21s/it]\n",
      "  0%|                                                                                           | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  36\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:27<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           6\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (30, 6)\n",
      "Data saved for larsen%20toubro .\n",
      "Nifty50 Extraction Search Count : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for axis%20bank : 35\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:50<00:00,  1.45s/it]\n",
      "  0%|                                                                                          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  410\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████▉           | 353/410 [01:42<00:15,  3.71it/s]"
     ]
    }
   ],
   "source": [
    "# Scraping news articles for all the companies listed in Nifty50\n",
    "\n",
    "for i, sstring in enumerate(SearchString):\n",
    "    print(\"Nifty50 Extraction Search Count :\",i+1)\n",
    "    initialize(sstring)\n",
    "    getnewslinks()\n",
    "    getarticles()\n",
    "    chkdata()\n",
    "    savefile(sstring)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nifty50 Extraction Search Count : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for axis%20bank : 35\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:53<00:00,  1.53s/it]\n",
      "  0%|                                                                                          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  410\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 410/410 [03:21<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           124\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (286, 6)\n",
      "Data saved for axis%20bank .\n",
      "Nifty50 Extraction Search Count : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for airtel : 38\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 38/38 [00:59<00:00,  1.58s/it]\n",
      "  0%|                                                                                          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  449\n",
      "Oldest Available Article:  2019-10-21\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 449/449 [03:52<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           83\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (366, 6)\n",
      "Data saved for airtel .\n",
      "Nifty50 Extraction Search Count : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for asian%20paints : 18\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:26<00:00,  1.46s/it]\n",
      "  0%|                                                                                          | 0/205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  205\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 205/205 [01:02<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           71\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (134, 6)\n",
      "Data saved for asian%20paints .\n",
      "Nifty50 Extraction Search Count : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for maruti : 29\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:46<00:00,  1.60s/it]\n",
      "  0%|▏                                                                                 | 1/344 [00:00<01:09,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  344\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 344/344 [02:35<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           82\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (262, 6)\n",
      "Data saved for maruti .\n",
      "Nifty50 Extraction Search Count : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                               | 1/22 [00:00<00:04,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hcl : 22\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:29<00:00,  1.33s/it]\n",
      "  0%|▎                                                                                 | 1/256 [00:00<00:51,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  256\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 256/256 [01:12<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           72\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (184, 6)\n",
      "Data saved for hcl .\n",
      "Nifty50 Extraction Search Count : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for bajaj%20finance : 33\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:45<00:00,  1.37s/it]\n",
      "  0%|                                                                                          | 0/394 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  394\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 394/394 [02:49<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           105\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (289, 6)\n",
      "Data saved for bajaj%20finance .\n",
      "Nifty50 Extraction Search Count : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for sbi : 46\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:04<00:00,  1.40s/it]\n",
      "  0%|▏                                                                                 | 1/543 [00:00<01:51,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  543\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 543/543 [1:14:37<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           108\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (435, 6)\n",
      "Data saved for sbi .\n",
      "Nifty50 Extraction Search Count : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▌                                                                         | 1/8 [00:00<00:01,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for reddy : 8\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:11<00:00,  1.44s/it]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  95\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [01:09<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           16\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (79, 6)\n",
      "Data saved for reddy .\n",
      "Nifty50 Extraction Search Count : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for mahindra : 35\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:54<00:00,  1.57s/it]\n",
      "  0%|                                                                                          | 0/414 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  414\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 414/414 [03:24<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           94\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (320, 6)\n",
      "Data saved for mahindra .\n",
      "Nifty50 Extraction Search Count : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for nestle : 13\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:21<00:00,  1.64s/it]\n",
      "  0%|                                                                                          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  152\n",
      "Oldest Available Article:  2019-11-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 152/152 [00:54<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           56\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (96, 6)\n",
      "Data saved for nestle .\n",
      "Nifty50 Extraction Search Count : 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for sun%20pharma : 21\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:33<00:00,  1.58s/it]\n",
      "  0%|▎                                                                                 | 1/246 [00:00<00:49,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  246\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 246/246 [01:23<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           69\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (177, 6)\n",
      "Data saved for sun%20pharma .\n",
      "Nifty50 Extraction Search Count : 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for titan : 15\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:21<00:00,  1.44s/it]\n",
      "  0%|                                                                                          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  179\n",
      "Oldest Available Article:  2019-11-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           54\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (125, 6)\n",
      "Data saved for titan .\n",
      "Nifty50 Extraction Search Count : 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for tech%20mahindra : 21\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:30<00:00,  1.46s/it]\n",
      "  0%|                                                                                          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  251\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 251/251 [00:59<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           81\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (170, 6)\n",
      "Data saved for tech%20mahindra .\n",
      "Nifty50 Extraction Search Count : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ultratech : 12\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:16<00:00,  1.37s/it]\n",
      "  1%|▌                                                                                 | 1/134 [00:00<00:26,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  134\n",
      "Oldest Available Article:  2019-11-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 134/134 [00:39<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           46\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (88, 6)\n",
      "Data saved for ultratech .\n",
      "Nifty50 Extraction Search Count : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for wipro : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.54s/it]\n",
      "  0%|                                                                                           | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  68\n",
      "Oldest Available Article:  2019-10-24\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 68/68 [00:37<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           13\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (55, 6)\n",
      "Data saved for wipro .\n",
      "Nifty50 Extraction Search Count : 26\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c29bb436c118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nifty50 Extraction Search Count :\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mgetnewslinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mgetarticles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-14cc35bc9134>\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(SearchString)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.thehindu.com/search/?q=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mSearchString\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&order=DESC&sort=publishdate&pd=year&ct=text&s=business&page=\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mresult_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data-page-no'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscript\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a.page-link'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0murl_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_max\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mheadlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Scraping news articles for all the companies listed in Nifty50\n",
    "\n",
    "for i, sstring in enumerate(SearchString):\n",
    "    if i >= 10:\n",
    "        print(\"Nifty50 Extraction Search Count :\",i+1)\n",
    "        initialize(sstring)\n",
    "        getnewslinks()\n",
    "        getarticles()\n",
    "        chkdata()\n",
    "        savefile(sstring)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nifty50 Extraction Search Count : 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for britannia : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72it/s]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  7\n",
      "Oldest Available Article:  2019-12-17\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:06<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           4\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (3, 6)\n",
      "Data saved for britannia .\n",
      "Nifty50 Extraction Search Count : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hdfc%20insurance : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.51s/it]\n",
      "  0%|                                                                                           | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  66\n",
      "Oldest Available Article:  2019-10-30\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:54<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           38\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (28, 6)\n",
      "Data saved for hdfc%20insurance .\n",
      "Nifty50 Extraction Search Count : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for powergrid : 17\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:23<00:00,  1.39s/it]\n",
      "  0%|                                                                                          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  199\n",
      "Oldest Available Article:  2019-10-24\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 199/199 [02:28<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           60\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (139, 6)\n",
      "Data saved for powergrid .\n",
      "Nifty50 Extraction Search Count : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ntpc : 19\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:26<00:00,  1.41s/it]\n",
      "  0%|▎                                                                                 | 1/226 [00:00<00:46,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  226\n",
      "Oldest Available Article:  2019-10-25\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 226/226 [01:58<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           71\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (155, 6)\n",
      "Data saved for ntpc .\n",
      "Nifty50 Extraction Search Count : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hero%20motocorp : 14\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:19<00:00,  1.38s/it]\n",
      "  0%|                                                                                          | 0/162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  162\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 162/162 [01:26<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           34\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (128, 6)\n",
      "Data saved for hero%20motocorp .\n",
      "Nifty50 Extraction Search Count : 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for cipla : 2\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  18\n",
      "Oldest Available Article:  2020-02-09\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:11<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           6\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (12, 6)\n",
      "Data saved for cipla .\n",
      "Nifty50 Extraction Search Count : 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for divis : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  2\n",
      "Oldest Available Article:  2019-11-05\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           0\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (2, 6)\n",
      "Data saved for divis .\n",
      "Nifty50 Extraction Search Count : 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for bajaj%20auto : 22\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:35<00:00,  1.59s/it]\n",
      "  0%|                                                                                          | 0/258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  258\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [01:42<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           105\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (153, 6)\n",
      "Data saved for bajaj%20auto .\n",
      "Nifty50 Extraction Search Count : 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for bajaj%20finserv : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:07<00:00,  1.29s/it]\n",
      "  0%|                                                                                           | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  66\n",
      "Oldest Available Article:  2019-12-14\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:24<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           24\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (42, 6)\n",
      "Data saved for bajaj%20finserv .\n",
      "Nifty50 Extraction Search Count : 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for sbi%20insurance : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:08<00:00,  1.50s/it]\n",
      "  0%|                                                                                           | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  67\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:30<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           27\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (40, 6)\n",
      "Data saved for sbi%20insurance .\n",
      "Nifty50 Extraction Search Count : 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for eicher : 2\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.54it/s]\n",
      "  5%|███▉                                                                               | 1/21 [00:00<00:03,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  21\n",
      "Oldest Available Article:  2020-01-01\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:12<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           4\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (17, 6)\n",
      "Data saved for eicher .\n",
      "Nifty50 Extraction Search Count : 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for indusind : 31\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:44<00:00,  1.45s/it]\n",
      "  0%|                                                                                          | 0/370 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  370\n",
      "Oldest Available Article:  2019-10-24\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 370/370 [02:26<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines            0\n",
      "Sections             0\n",
      "Articles           100\n",
      "Published_Dates      0\n",
      "Source_URLs          0\n",
      "ByLines              0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (270, 6)\n",
      "Data saved for indusind .\n",
      "Nifty50 Extraction Search Count : 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for grasim : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:00<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  5\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           0\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (5, 6)\n",
      "Data saved for grasim .\n",
      "Nifty50 Extraction Search Count : 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for bpcl : 8\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:12<00:00,  1.56s/it]\n",
      "  0%|                                                                                           | 0/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  88\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [01:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           14\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (74, 6)\n",
      "Data saved for bpcl .\n",
      "Nifty50 Extraction Search Count : 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for jsw : 4\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "  0%|                                                                                           | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  39\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [00:25<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           1\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (38, 6)\n",
      "Data saved for jsw .\n",
      "Nifty50 Extraction Search Count : 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for upl : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  3\n",
      "Oldest Available Article:  2020-05-20\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           0\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (3, 6)\n",
      "Data saved for upl .\n",
      "Nifty50 Extraction Search Count : 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for shree%20cement : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  5\n",
      "Oldest Available Article:  2019-11-29\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           3\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (2, 6)\n",
      "Data saved for shree%20cement .\n",
      "Nifty50 Extraction Search Count : 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for tata%20steel : 27\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:39<00:00,  1.45s/it]\n",
      "  0%|                                                                                          | 0/322 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  322\n",
      "Oldest Available Article:  2019-10-24\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 322/322 [02:01<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           88\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (234, 6)\n",
      "Data saved for tata%20steel .\n",
      "Nifty50 Extraction Search Count : 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for hindalco : 1\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  9\n",
      "Oldest Available Article:  2019-11-11\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:05<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           0\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (9, 6)\n",
      "Data saved for hindalco .\n",
      "Nifty50 Extraction Search Count : 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for adani : 7\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:09<00:00,  1.38s/it]\n",
      "  0%|                                                                                           | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  84\n",
      "Oldest Available Article:  2019-10-29\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [00:51<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           13\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (71, 6)\n",
      "Data saved for adani .\n",
      "Nifty50 Extraction Search Count : 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ongc : 23\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:29<00:00,  1.30s/it]\n",
      "  0%|▎                                                                                 | 1/272 [00:00<00:54,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  272\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 272/272 [01:43<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           73\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (199, 6)\n",
      "Data saved for ongc .\n",
      "Nifty50 Extraction Search Count : 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for coal%20india : 9\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:12<00:00,  1.35s/it]\n",
      "  0%|                                                                                          | 0/105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  105\n",
      "Oldest Available Article:  2019-10-31\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [01:02<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           24\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (81, 6)\n",
      "Data saved for coal%20india .\n",
      "Nifty50 Extraction Search Count : 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for tata%20motors : 17\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:22<00:00,  1.34s/it]\n",
      "  1%|▍                                                                                 | 1/196 [00:00<00:41,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  196\n",
      "Oldest Available Article:  2019-10-22\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [01:38<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           39\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (157, 6)\n",
      "Data saved for tata%20motors .\n",
      "Nifty50 Extraction Search Count : 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for ioc : 6\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:08<00:00,  1.45s/it]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  64\n",
      "Oldest Available Article:  2019-10-23\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:25<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines           0\n",
      "Sections            0\n",
      "Articles           10\n",
      "Published_Dates     0\n",
      "Source_URLs         0\n",
      "ByLines             0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (54, 6)\n",
      "Data saved for ioc .\n",
      "Nifty50 Extraction Search Count : 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of result pages for gail : 3\n",
      "[INFO] Extracting Links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.03s/it]\n",
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Links Extracted.\n",
      "Total No. of Pages to be Scraped =  27\n",
      "Oldest Available Article:  2020-01-08\n",
      "[INFO] Extracting Articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:12<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Articles Extracted.\n",
      "Missing Info in Scraped Data :\n",
      "Headlines          0\n",
      "Sections           0\n",
      "Articles           5\n",
      "Published_Dates    0\n",
      "Source_URLs        0\n",
      "ByLines            0\n",
      "dtype: int64\n",
      "Total Usable Scraped Data :  (22, 6)\n",
      "Data saved for gail .\n"
     ]
    }
   ],
   "source": [
    "# Scraping news articles for all the companies listed in Nifty50\n",
    "\n",
    "for i, sstring in enumerate(SearchString):\n",
    "    if i >= 25:\n",
    "        print(\"Nifty50 Extraction Search Count :\",i+1)\n",
    "        initialize(sstring)\n",
    "        getnewslinks()\n",
    "        getarticles()\n",
    "        chkdata()\n",
    "        savefile(sstring)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pages with blank Articles have a different structure.\n",
    "# These pages have a byline 'The Hindu Net Desk' and can be separately scraped and preprocessed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
